{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57f70e22",
   "metadata": {},
   "source": [
    "# Task 2. Performance Comparison of `HyperLogLog` with Exact Unique Element Counting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a57403a",
   "metadata": {},
   "source": [
    "Create a script to compare exact unique element counting with counting using `HyperLogLog`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Module for comparing exact unique counting vs. HyperLogLog on IP logs.\n",
    "\n",
    "This notebook provides:\n",
    "  - Loading IPs from a log file.\n",
    "  - Exact unique counting with a set using parallel processing.\n",
    "  - Approximate unique counting with HyperLogLog.\n",
    "  - A performance comparison including execution time, memory usage, and error.\n",
    "\n",
    "Follows Google Python Style Guide:\n",
    "  https://google.github.io/styleguide/pyguide.html\n",
    "\"\"\"\n",
    "\n",
    "# Standard library imports\n",
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "import multiprocessing as mp\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import tempfile\n",
    "import os\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from pathlib import Path\n",
    "from typing import Iterator, List, Optional, Set\n",
    "import random\n",
    "import ipaddress\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "# Third-party imports\n",
    "import mmh3\n",
    "import pandas as pd\n",
    "\n",
    "IP_PATTERN = re.compile(r\"(\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a136c1",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f257255",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperLogLog:\n",
    "    \"\"\"A HyperLogLog implementation for cardinality estimation.\n",
    "\n",
    "    This class provides a memory-efficient way to estimate the number of unique\n",
    "    elements in a large dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, p: int = 14):\n",
    "        \"\"\"Initializes the HyperLogLog structure.\n",
    "\n",
    "        Args:\n",
    "            p: The precision parameter, which defines the number of registers as m = 2^p.\n",
    "               A higher 'p' increases accuracy at the cost of memory.\n",
    "               Valid range is 4 <= p <= 16.\n",
    "        \"\"\"\n",
    "        if not 4 <= p <= 16:\n",
    "            raise ValueError(\"Precision 'p' must be between 4 and 16.\")\n",
    "        self.p: int = p\n",
    "        self.m: int = 1 << p  # Number of registers, e.g., 2^14 = 16384\n",
    "        self.registers: List[int] = [0] * self.m\n",
    "        self.alpha: float = self._get_alpha()\n",
    "\n",
    "    def _get_alpha(self) -> float:\n",
    "        \"\"\"Calculates the alpha constant for bias correction.\"\"\"\n",
    "        if self.m == 16: # p=4\n",
    "            return 0.673\n",
    "        if self.m == 32: # p=5\n",
    "            return 0.697\n",
    "        if self.m == 64: # p=6\n",
    "            return 0.709\n",
    "        # General formula for other values of m\n",
    "        return 0.7213 / (1 + 1.079 / self.m)\n",
    "\n",
    "    def add(self, item: str) -> None:\n",
    "        \"\"\"Adds an item to the HyperLogLog counter.\n",
    "\n",
    "        Args:\n",
    "            item: The item to add to the set.\n",
    "        \"\"\"\n",
    "        hashed_item: int = mmh3.hash(str(item), signed=False)\n",
    "        register_index: int = hashed_item & (self.m - 1)\n",
    "        bits_for_rho: int = hashed_item >> self.p\n",
    "\n",
    "        if bits_for_rho == 0:\n",
    "            rho: int = 32 - self.p + 1\n",
    "        else:\n",
    "            rho: int = (32 - self.p) - bits_for_rho.bit_length() + 1\n",
    "\n",
    "        self.registers[register_index] = max(self.registers[register_index], rho)\n",
    "\n",
    "    def count(self) -> float:\n",
    "        \"\"\"Estimates the cardinality of the set.\n",
    "        \n",
    "        Returns:\n",
    "            Estimated number of unique elements.\n",
    "        \"\"\"\n",
    "        Z: float = 1.0 / sum(2.0 ** -register for register in self.registers)\n",
    "        raw_estimate: float = self.alpha * (self.m**2) * Z\n",
    "\n",
    "        if raw_estimate <= 2.5 * self.m:\n",
    "            zero_registers: int = self.registers.count(0)\n",
    "            if zero_registers > 0:\n",
    "                return self.m * math.log(self.m / zero_registers)\n",
    "\n",
    "        return raw_estimate\n",
    "    def merge(self, other: 'HyperLogLog') -> None:\n",
    "        \"\"\"Merges another HyperLogLog instance into this one.\n",
    "\n",
    "        Args:\n",
    "            other: Another HyperLogLog instance to merge.\n",
    "        \n",
    "        Raises:\n",
    "            ValueError: If the 'p' parameters of the two instances do not match.\n",
    "        \"\"\"\n",
    "        if self.p != other.p:\n",
    "            raise ValueError(\"To merge HyperLogLog instances, the 'p' parameters must be the same.\")\n",
    "\n",
    "        for i in range(self.m):\n",
    "            self.registers[i] = max(self.registers[i], other.registers[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67318903",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunk_hll(chunk_data: tuple[Path, int, int]) -> HyperLogLog:\n",
    "    \"\"\"Processes a chunk of the file and returns a HyperLogLog instance for that chunk.\n",
    "    \n",
    "    Args:\n",
    "        chunk_data: A tuple containing the file path, start byte, and end byte.\n",
    "    \n",
    "    Returns:\n",
    "        A HyperLogLog instance with the processed data from the chunk.\n",
    "    \"\"\"\n",
    "    file_path, start, end = chunk_data\n",
    "    hll: HyperLogLog = HyperLogLog(p=14)\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            f.seek(start)\n",
    "            if start != 0:\n",
    "                f.readline()\n",
    "\n",
    "            while f.tell() < end:\n",
    "                line: str = f.readline()\n",
    "                if not line:\n",
    "                    break\n",
    "                match: Optional[re.Match] = IP_PATTERN.search(line)\n",
    "                if match:\n",
    "                    hll.add(match.group(1))\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file chunk for HLL: {e}\")\n",
    "    return hll\n",
    "\n",
    "\n",
    "def get_hll_count_parallel(file_path: Path, num_workers: int) -> float:\n",
    "    \"\"\"Estimates cardinality using HLL in parallel.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the log file.\n",
    "        num_workers: Number of parallel workers to use.\n",
    "    \n",
    "    Returns:\n",
    "        Estimated number of unique elements using HyperLogLog.\n",
    "    \"\"\"\n",
    "    file_size: int = file_path.stat().st_size\n",
    "    chunk_size: int = file_size // num_workers\n",
    "\n",
    "    chunks: list[tuple[Path, int, int]] = []\n",
    "    for i in range(num_workers):\n",
    "        start: int = i * chunk_size\n",
    "        end: int = (i + 1) * chunk_size if i < num_workers - 1 else file_size\n",
    "        chunks.append((file_path, start, end))\n",
    "\n",
    "    final_hll: HyperLogLog = HyperLogLog(p=14)\n",
    "    with ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
    "        results = executor.map(process_chunk_hll, chunks)\n",
    "\n",
    "        for hll_chunk in results:\n",
    "            final_hll.merge(hll_chunk)\n",
    "\n",
    "    return final_hll.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3e32e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ips_from_log_stream(file_path: Path) -> Iterator[str]:\n",
    "    \"\"\"Extracts IP addresses from a log file by searching each line.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the log file.\n",
    "    \n",
    "    Yields:\n",
    "        IP addresses found in the log file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                match: Optional[re.Match] = IP_PATTERN.search(line)\n",
    "                if match:\n",
    "                    yield match.group(1)\n",
    "    except FileNotFoundError:\n",
    "        return\n",
    "\n",
    "def process_chunk(chunk_data: tuple[Path, int, int]) -> set[str]:\n",
    "    \"\"\"Processes a chunk of a log file to extract unique IP addresses.\n",
    "    \n",
    "    Args:\n",
    "        chunk_data: A tuple containing (file_path, start_byte, end_byte).\n",
    "    \n",
    "    Returns:\n",
    "        A set of unique IP addresses found in the chunk.\n",
    "    \"\"\"\n",
    "    file_path, start, end = chunk_data\n",
    "    unique_ips: Set[str] = set()\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            f.seek(start)\n",
    "            if start != 0:\n",
    "                f.readline()\n",
    "\n",
    "            while f.tell() < end:\n",
    "                line: str = f.readline()\n",
    "                if not line:\n",
    "                    break\n",
    "                match: Optional[re.Match] = IP_PATTERN.search(line)\n",
    "                if match:\n",
    "                    unique_ips.add(match.group(1))\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing chunk: {e}\")\n",
    "    return unique_ips\n",
    "\n",
    "def get_unique_ips_parallel(file_path: Path, num_workers: int) -> set[str]:\n",
    "    \"\"\"Extracts unique IPs from a log file using parallel processing.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the log file.\n",
    "        num_workers: Number of parallel workers to use.\n",
    "    \n",
    "    Returns:\n",
    "        A set of unique IP addresses found in the log file.\n",
    "    \"\"\"\n",
    "    file_size: int = file_path.stat().st_size\n",
    "    chunk_size: int = file_size // num_workers\n",
    "    \n",
    "    chunks = []\n",
    "    for i in range(num_workers):\n",
    "        start: int = i * chunk_size\n",
    "        end: int = (i + 1) * chunk_size if i < num_workers - 1 else file_size\n",
    "        chunks.append((file_path, start, end))\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
    "        results: Iterator[set[str]] = executor.map(process_chunk, chunks)\n",
    "\n",
    "        total_unique_ips: Set[str] = set()\n",
    "        for ip_set in results:\n",
    "            total_unique_ips.update(ip_set)\n",
    "\n",
    "    return total_unique_ips"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b54ec37",
   "metadata": {},
   "source": [
    "### Additional generating large log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d386a599",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_ip() -> str:\n",
    "    \"\"\"Generates a random IPv4 address.\"\"\"\n",
    "    return str(ipaddress.IPv4Address(random.randint(0, 2**32 - 1)))\n",
    "\n",
    "\n",
    "def generate_log_entry(ip_address: str) -> str:\n",
    "    \"\"\"Creates a single log line.\"\"\"\n",
    "    timestamp: str = (datetime.now() - timedelta(seconds=random.randint(0, 86400))).strftime(\n",
    "        \"%d/%b/%Y:%H:%M:%S %z\"\n",
    "    )\n",
    "    methods: List[str] = [\"GET\", \"POST\", \"PUT\", \"DELETE\"]\n",
    "    paths: List[str] = [\n",
    "        \"/index.html\",\n",
    "        \"/api/data\",\n",
    "        \"/assets/style.css\",\n",
    "        \"/images/logo.png\",\n",
    "        \"/admin/login\",\n",
    "    ]\n",
    "    statuses: List[int] = [200, 201, 404, 500, 302]\n",
    "    user_agents: List[str] = [\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15\",\n",
    "        \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:89.0) Gecko/20100101 Firefox/89.0\",\n",
    "    ]\n",
    "\n",
    "    return f'{ip_address} - - [{timestamp}] \"{random.choice(methods)} {random.choice(paths)} HTTP/1.1\" {random.choice(statuses)} {random.randint(100, 5000)} \"-\" \"{random.choice(user_agents)}\"'\n",
    "\n",
    "\n",
    "def generate_large_log_file(file_path: Path, num_unique_ips: int = 100_000, num_log_entries: int = 1_000_000) -> None:\n",
    "    \"\"\"Generates a large log file with random data.\n",
    "\n",
    "    Args:\n",
    "        file_path: The path to the file for saving logs.\n",
    "        num_unique_ips: The number of unique IP addresses.\n",
    "        num_log_entries: The total number of log entries.\n",
    "    \"\"\"\n",
    "    print(f\"Generating {num_unique_ips} unique IP addresses...\")\n",
    "    unique_ips: List[str] = [generate_random_ip() for _ in range(num_unique_ips)]\n",
    "\n",
    "    print(f\"Generating {num_log_entries} log entries into file '{file_path}'...\")\n",
    "    with open(file_path, \"w\") as f:\n",
    "        for i in range(num_log_entries):\n",
    "            if random.random() < 0.8:\n",
    "                ip: str = random.choice(unique_ips)\n",
    "            else:\n",
    "                ip: str = generate_random_ip()\n",
    "\n",
    "            log_entry: str = generate_log_entry(ip)\n",
    "            f.write(log_entry + \"\\n\")\n",
    "            if (i + 1) % (num_log_entries // 10) == 0:\n",
    "                print(f\"  ...generated {i + 1} entries\")\n",
    "\n",
    "    print(\"Log file generation complete.\")\n",
    "\n",
    "\n",
    "def test_large_dataset(num_unique_ips: int = 150_000, num_log_entries: int = 2_000_000):\n",
    "    \"\"\"Runs testing on a large dataset using a temporary file.\n",
    "\n",
    "    Args:\n",
    "        num_unique_ips: Number of unique IP addresses to generate.\n",
    "        num_log_entries: Total number of log entries to generate.\n",
    "    \"\"\"\n",
    "\n",
    "    temp_dir: str = tempfile.gettempdir()\n",
    "    log_file_path: Path = Path(temp_dir) / \"large_access_temp.log\"\n",
    "\n",
    "    try:\n",
    "        generate_large_log_file(log_file_path, num_unique_ips, num_log_entries)\n",
    "\n",
    "        print(\"\\nStarting parallel processing on the large log file...\")\n",
    "\n",
    "        num_workers: int = mp.cpu_count()\n",
    "\n",
    "        # Paralel precise counting\n",
    "        start_time_exact: float = time.time()\n",
    "        unique_ips_set: set[str] = get_unique_ips_parallel(log_file_path, num_workers)\n",
    "        exact_count: int = len(unique_ips_set)\n",
    "        exact_time: float = time.time() - start_time_exact\n",
    "\n",
    "        # Paralel HLL counting\n",
    "        start_time_hll: float = time.time()\n",
    "        hll_count: float = get_hll_count_parallel(log_file_path, num_workers)\n",
    "        hll_time: float = time.time() - start_time_hll\n",
    "\n",
    "        if exact_count > 0:\n",
    "            error: float = abs(exact_count - hll_count) / exact_count * 100\n",
    "            exact_mem: int = sys.getsizeof(unique_ips_set)\n",
    "            # memory used by HLL registers remains fixed\n",
    "            hll_mem: int = sys.getsizeof(HyperLogLog(p=14).registers)\n",
    "\n",
    "            results: dict[str, list[str | float | int]] = {\n",
    "                \"Method\": [\"Exact Count (Parallel)\", \"HyperLogLog (Parallel)\"],\n",
    "                \"Unique Elements\": [f\"{exact_count}\", f\"{int(hll_count)}\"],\n",
    "                \"Execution Time (s)\": [f\"{exact_time:.4f}\", f\"{hll_time:.4f}\"],\n",
    "                \"Memory Usage (bytes)\": [f\"{exact_mem}\", f\"{hll_mem}\"],\n",
    "                \"Error\": [\"0.00%\", f\"{error:.2f}%\"],\n",
    "            }\n",
    "\n",
    "            df: pd.DataFrame = pd.DataFrame(results)\n",
    "            print(\"\\nComparison Results:\")\n",
    "            display(df)\n",
    "        else:\n",
    "            print(\n",
    "                \"\\nCould not find any IP addresses. Please verify the log file format and the IP_PATTERN regex.\"\n",
    "            )\n",
    "\n",
    "    finally:\n",
    "        if os.path.exists(log_file_path):\n",
    "            os.remove(log_file_path)\n",
    "            print(f\"\\nTemporary file '{log_file_path}' has been deleted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1441afbc",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d90e770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparison Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>Unique Elements</th>\n",
       "      <th>Execution Time (s)</th>\n",
       "      <th>Memory Usage (bytes)</th>\n",
       "      <th>Error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Exact Count (Set)</td>\n",
       "      <td>28</td>\n",
       "      <td>0.0786</td>\n",
       "      <td>1240</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HyperLogLog</td>\n",
       "      <td>28</td>\n",
       "      <td>0.0665</td>\n",
       "      <td>131128</td>\n",
       "      <td>0.09%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Method Unique Elements Execution Time (s) Memory Usage (bytes)  \\\n",
       "0  Exact Count (Set)              28             0.0786                 1240   \n",
       "1        HyperLogLog              28             0.0665               131128   \n",
       "\n",
       "   Error  \n",
       "0  0.00%  \n",
       "1  0.09%  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 10500000 unique IP addresses...\n",
      "Generating 200000000 log entries into file '/tmp/large_access_temp.log'...\n",
      "  ...generated 20000000 entries\n",
      "  ...generated 40000000 entries\n",
      "  ...generated 60000000 entries\n",
      "  ...generated 80000000 entries\n",
      "  ...generated 100000000 entries\n",
      "  ...generated 120000000 entries\n",
      "  ...generated 140000000 entries\n",
      "  ...generated 160000000 entries\n",
      "  ...generated 180000000 entries\n",
      "  ...generated 200000000 entries\n",
      "Log file generation complete.\n",
      "\n",
      "Starting parallel processing on the large log file...\n",
      "\n",
      "Comparison Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>Unique Elements</th>\n",
       "      <th>Execution Time (s)</th>\n",
       "      <th>Memory Usage (bytes)</th>\n",
       "      <th>Error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Exact Count (Parallel)</td>\n",
       "      <td>50211427</td>\n",
       "      <td>86.6692</td>\n",
       "      <td>2147483864</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HyperLogLog (Parallel)</td>\n",
       "      <td>50616234</td>\n",
       "      <td>76.3209</td>\n",
       "      <td>131128</td>\n",
       "      <td>0.81%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Method Unique Elements Execution Time (s)  \\\n",
       "0  Exact Count (Parallel)        50211427            86.6692   \n",
       "1  HyperLogLog (Parallel)        50616234            76.3209   \n",
       "\n",
       "  Memory Usage (bytes)  Error  \n",
       "0           2147483864  0.00%  \n",
       "1               131128  0.81%  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Temporary file '/tmp/large_access_temp.log' has been deleted.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    LOG_FILE_PATH = Path(\"../data/lms-stage-access.log\")\n",
    "\n",
    "    num_workers: int = mp.cpu_count()\n",
    "    start_time_exact: float = time.time()\n",
    "    unique_ips_set: set[str] = get_unique_ips_parallel(LOG_FILE_PATH, num_workers)\n",
    "    exact_count: int = len(unique_ips_set)\n",
    "    exact_time: float = time.time() - start_time_exact\n",
    "\n",
    "    hll = HyperLogLog(p=14)\n",
    "    start_time_hll: float = time.time()\n",
    "    ip_stream: Iterator[str] = get_ips_from_log_stream(LOG_FILE_PATH)\n",
    "    for ip in ip_stream:\n",
    "        hll.add(ip)\n",
    "    hll_count: float = hll.count()\n",
    "    hll_time: float = time.time() - start_time_hll\n",
    "\n",
    "    if exact_count > 0:\n",
    "        error: float = abs(exact_count - hll_count) / exact_count * 100\n",
    "        exact_mem: int = sys.getsizeof(unique_ips_set)\n",
    "        hll_mem: int = sys.getsizeof(hll.registers)\n",
    "\n",
    "        results: dict[str, list[str | float | int]] = {\n",
    "            \"Method\": [\"Exact Count (Set)\", \"HyperLogLog\"],\n",
    "            \"Unique Elements\": [f\"{exact_count}\", f\"{int(hll_count)}\"],\n",
    "            \"Execution Time (s)\": [f\"{exact_time:.4f}\", f\"{hll_time:.4f}\"],\n",
    "            \"Memory Usage (bytes)\": [f\"{exact_mem}\", f\"{hll_mem}\"],\n",
    "            \"Error\": [\"0.00%\", f\"{error:.2f}%\"],\n",
    "        }\n",
    "\n",
    "        df: pd.DataFrame = pd.DataFrame(results)\n",
    "        print(\"\\nComparison Results:\")\n",
    "        display(df)\n",
    "    else:\n",
    "        print(\"\\nCould not find any IP addresses. Please verify the log file format and the IP_PATTERN regex.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Main task\n",
    "    main()\n",
    "    # Additional test on the large dataset\n",
    "    test_large_dataset(num_unique_ips=10_500_000, num_log_entries=200_000_000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "goit-algo2-hw-06-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
